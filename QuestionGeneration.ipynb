{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtBuup/WvsaXeRfZl+2vIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "facde541435241d2a630c11146083d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e5d802f3a07a476baab1f70217c746a5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a5abe97c29864112b952b5493963e540",
              "IPY_MODEL_6b794758e520418b97a706e195780b69",
              "IPY_MODEL_b63668af77a347578539b6fbf13a7120"
            ]
          }
        },
        "e5d802f3a07a476baab1f70217c746a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5abe97c29864112b952b5493963e540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_73f4ccc0d0fd4d8887a6d769c8b55b31",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc148fb25f004cbc9d9934c567710eb6"
          }
        },
        "6b794758e520418b97a706e195780b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90e0ef265100429f8908adf1e73e59a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760289,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760289,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_046b5cebb7264fb6a99aba34fbb44ca4"
          }
        },
        "b63668af77a347578539b6fbf13a7120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea3fc97cb36a4aee9ae872ed82c6faec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760k/760k [00:00&lt;00:00, 1.18MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7f38bbf21454c50bf3b681e7f2940e0"
          }
        },
        "73f4ccc0d0fd4d8887a6d769c8b55b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc148fb25f004cbc9d9934c567710eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90e0ef265100429f8908adf1e73e59a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "046b5cebb7264fb6a99aba34fbb44ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea3fc97cb36a4aee9ae872ed82c6faec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7f38bbf21454c50bf3b681e7f2940e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8eed8c7292404ee78458823ee54221d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c0c7715aa32e418290b5fda4b9895ab6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a911a1f9fe524fa58fab293e49abcb29",
              "IPY_MODEL_4b45393c1f374b87a429448753994c2b",
              "IPY_MODEL_cbb3b4a323434880bf859395c61cf232"
            ]
          }
        },
        "c0c7715aa32e418290b5fda4b9895ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a911a1f9fe524fa58fab293e49abcb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d9868bb0e5443d0a397f924e421a28a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_860df6a688e14ce68d7ae7edaae0fac9"
          }
        },
        "4b45393c1f374b87a429448753994c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f64d67297442433a90192de29c2774dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1312669,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1312669,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1296c7edfee41f587ae0fc1a6713916"
          }
        },
        "cbb3b4a323434880bf859395c61cf232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08ef4b812b7842899454ef33bf9d3c0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.31M/1.31M [00:00&lt;00:00, 7.03MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40e64320e9304d82970b9cb15af4f08f"
          }
        },
        "7d9868bb0e5443d0a397f924e421a28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "860df6a688e14ce68d7ae7edaae0fac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f64d67297442433a90192de29c2774dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1296c7edfee41f587ae0fc1a6713916": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08ef4b812b7842899454ef33bf9d3c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40e64320e9304d82970b9cb15af4f08f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterryjack/question_generation/blob/main/QuestionGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_SPXgui_aIj"
      },
      "source": [
        "# Natural Language Question Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAsz2WI1jnw-"
      },
      "source": [
        "from typing import List, Tuple, Optional"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7enhIp0jMmV"
      },
      "source": [
        "# 1. Baseline (Heuristic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWLdPkcjU4f"
      },
      "source": [
        "**Description**:\n",
        "\n",
        "Our baseline method (to compare all future approaches against) is a very simple heuristic which randomly chooses a seed token from a predefined list of words typically found at the beginning of generic questions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "teeIaHP2jRSe",
        "outputId": "b5526cfa-da9c-49e6-fe12-9727651a83ee"
      },
      "source": [
        "from random import choice\n",
        "\n",
        "seed_tokens = [\"can\",\"what\",\"when\",\"why\"]\n",
        "seed_question = choice(seed_tokens)\n",
        "seed_question"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'why'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikoiokdgjb0n"
      },
      "source": [
        "The keywords are then extracted from the utterances using a simple `stopword` filter (We use `NLTK` for its inbuilt stopword list)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQd5ntkXjhw8",
        "outputId": "2a7042ad-9af7-4e2f-a067-c4dba465aff8"
      },
      "source": [
        "from nltk import download\n",
        "download('stopwords')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnJyB3dJjaaK",
        "outputId": "472a8217-e17c-475c-d701-a7a94d0ca0c7"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopword_tokens = set(stopwords.words('english'))\n",
        "\n",
        "def extract_keywords(text:str) -> List[str]:\n",
        "    return list(filter(lambda token:token not in stopword_tokens, text.split()))\n",
        "\n",
        "keywords = extract_keywords(\"this is an example\")\n",
        "keywords"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['example']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwStGptmj4rv"
      },
      "source": [
        "A couple of keywords are randomly sampled and appended to the seed question to ensure it has some superficial level of coherence \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "itOe6bV8j6Rl",
        "outputId": "7a7f213a-466f-4a02-af44-289cf2a4aca9"
      },
      "source": [
        "from random import sample\n",
        "\n",
        "number_of_samples = min(len(keywords),2)\n",
        "seed_question += f\" {' '.join(sample(keywords,number_of_samples))}\"\n",
        "seed_question"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'why example'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUh_29ikb0K"
      },
      "source": [
        "This can sound unnatural and so the final step is to translate the question into another language (e.g. arabic) and back into english to encourage more natural phrasing (We use `Textblob`'s inbuilt translate function).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM4WKaUKkyJ2",
        "outputId": "2b54a8ee-8cfe-47b9-8065-bfd55a98a226"
      },
      "source": [
        "download('punkt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1tN8jYhkmlB"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob.exceptions import NotTranslated\n",
        "\n",
        "def rephrase_for_fluency(text:str) -> str:\n",
        "    analysis = TextBlob(text)\n",
        "    try:\n",
        "        return ' '.join(analysis.translate(to='ar').translate(to='en').words) + \"?\"\n",
        "    except NotTranslated:\n",
        "        return text\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBI-fdLekqJx"
      },
      "source": [
        "This method is known as `spinning` and can modify words (e.g. like -> love)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rVECoEiskppc",
        "outputId": "2df1fb80-f159-418c-aeb2-05688286a087"
      },
      "source": [
        "rephrase_for_fluency(\"i like cats\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I love cats?'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8N6HqrFlFdH"
      },
      "source": [
        "def quick_generate(text:str) -> str:\n",
        "  question = extract_seed_question(text)\n",
        "  return rephrase_for_fluency(question) \n",
        "\n",
        "def extract_seed_question(text:str) -> str:\n",
        "  seed_question = choice(seed_tokens)\n",
        "  keywords = extract_keywords(text)\n",
        "  if any(keywords):\n",
        "    number_of_samples = min(len(keywords),2)\n",
        "    seed_question += f\" {' '.join(sample(keywords,number_of_samples))}\"\n",
        "  seed_question += \" ?\"\n",
        "  return seed_question"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xGxCajxplh9w",
        "outputId": "f482c5f7-eb68-4e69-af1b-4902cf68b587"
      },
      "source": [
        "quick_generate(\"i like cats\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What do cats like?'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsbkuWMHk50a"
      },
      "source": [
        "**Pros**:\n",
        "\n",
        "- The baseline method does not require any training to begin using \n",
        "- it is a lightweight solution\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "- Although spinning produces a natural sounding question, it does not ensure that the final question makes sense nor that it sounds intelligent (e.g. \"Can you love cats?\" is a strange question to ask).  \n",
        "- This method will not introduce any semantic novelty into the question (the meaning of the question will be nearly identical to the original utterances) thus creating a coherent yet dull question overall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "hGjNInB9omYn",
        "outputId": "688e6704-e690-4de2-daea-2bf44b75c68d"
      },
      "source": [
        "while True:\n",
        "  print(quick_generate(input(\">\")))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">this is a test\n",
            "can you test?\n",
            ">i live near a forest\n",
            "Which forest do you live in?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e94368027f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquick_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj7lTGdAluBE"
      },
      "source": [
        "# 2. Albert Mask Token (Pre-trained)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGlY5zaSlzIl"
      },
      "source": [
        "The next method maintains the main advantage of the baseline (i.e. it does not need training) yet improves upon it by being able to generate words and, thus, add novelty into the semantics of the question.  To generate questions, we use a pre-trained instance of `Albert` (Albert stands for \"A lite BERT\") as it promises to have similar performance to `BERT` while being lighter.  \n",
        "\n",
        "The reason we use Albert is to do with the tasks it was trained on.  Most pretrained Generative language models (e.g. `GPT3`, `GPT-Neo`, `T5`, Bert, etc) are able to generate text, but they are not specifically trained to generate questions.  Now Albert is no exception, however, unlike T5 and the GPT family, BERT (and Albert by extension) are trained on a specific task which does make it possible for us to generate questions without having to retrain of fine-tune it at all!  This is the Masked Token Prediction task, \n",
        "\n",
        "We can use Albert's ability to predict a masked token to trick it into inserting new words into a question, thus growing the sentence from within!  \n",
        "\n",
        "e.g.\n",
        "```python \n",
        "iteration 1:\n",
        "\tquestion: \"what cats?\"\n",
        "\tinserted mask: \"what [MASK] cats?\"\n",
        "\tpredicted word: \"what [cute] cats\"\n",
        "\n",
        "iteration 2:\n",
        "\tquestion: \"what cute cats?\"\n",
        "\tinserted mask: \"what [MASK] cute cats?\"\n",
        "\tpredicted word: \"what [are] cute cats?\"\n",
        "\n",
        "iteration 3:\n",
        "\tquestion: \"what are cute cats?\"\n",
        "\tinserted mask: \"what are cute cats [MASK]?\"\n",
        "\t...etc\n",
        "```\n",
        "\n",
        "This method of natural language generation via 'inserting' new words differs from most generation tasks (which involve predicting the next word or character at the end of a given sequence), but the reason we choose such a method is due to the convenience of constraining the generated text into a desired format (i.e. the form of a question) by specifying the initial and final tokens (e.g. \"what ... ?\"). \n",
        "\n",
        "\n",
        "First we construct the function which inserts the mask token somewhere inside the question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQfFOoQal6HQ"
      },
      "source": [
        "from random import choice\n",
        "\n",
        "mask_token = \"[MASK]\"\n",
        "\n",
        "def grow_question(question:str) -> str:\n",
        "    tokens = question.split()\n",
        "    index = choice(range(len(tokens)))\n",
        "    new_tokens = tokens[:index] + [mask_token] + tokens[index:]\n",
        "    return predict_masked_words(text=' '.join(new_tokens))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoLMvJDYmJr6"
      },
      "source": [
        "In order to predict the masked token, we need to load in the pretrained Albert model from Huggingface's `transformers` library and pass it the question containing the masked tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msYvdE3OmV_9",
        "outputId": "2535bb9e-62a3-4485-bb17-31b787872934"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 33.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxOWoHKjmfTD",
        "outputId": "2fb8c2f8-e9f7-46a7-edd3-96077634eb4b"
      },
      "source": [
        "!pip install SentencePiece"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SentencePiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: SentencePiece\n",
            "Successfully installed SentencePiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "facde541435241d2a630c11146083d41",
            "e5d802f3a07a476baab1f70217c746a5",
            "a5abe97c29864112b952b5493963e540",
            "6b794758e520418b97a706e195780b69",
            "b63668af77a347578539b6fbf13a7120",
            "73f4ccc0d0fd4d8887a6d769c8b55b31",
            "cc148fb25f004cbc9d9934c567710eb6",
            "90e0ef265100429f8908adf1e73e59a7",
            "046b5cebb7264fb6a99aba34fbb44ca4",
            "ea3fc97cb36a4aee9ae872ed82c6faec",
            "c7f38bbf21454c50bf3b681e7f2940e0",
            "8eed8c7292404ee78458823ee54221d3",
            "c0c7715aa32e418290b5fda4b9895ab6",
            "a911a1f9fe524fa58fab293e49abcb29",
            "4b45393c1f374b87a429448753994c2b",
            "cbb3b4a323434880bf859395c61cf232",
            "7d9868bb0e5443d0a397f924e421a28a",
            "860df6a688e14ce68d7ae7edaae0fac9",
            "f64d67297442433a90192de29c2774dc",
            "d1296c7edfee41f587ae0fc1a6713916",
            "08ef4b812b7842899454ef33bf9d3c0b",
            "40e64320e9304d82970b9cb15af4f08f"
          ]
        },
        "id": "FFEH0yVmmAWF",
        "outputId": "f87fcbe6-c9e5-4121-9060-b4e9a4fab02a"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
        "\n",
        "model_name = 'albert-base-v2'\n",
        "\n",
        "tokeniser = AlbertTokenizer.from_pretrained(model_name)\n",
        "model = AlbertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "def predict_masked_words(text:str) -> str:\n",
        "    inputs = tokeniser(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    \n",
        "    tokens = tokeniser.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "    predicted_tokens = tokeniser.convert_ids_to_tokens(outputs.logits[0].argmax(dim=1))\n",
        "    \n",
        "    replaced_tokens = replace_mask_tokens_with_predicted_tokens(tokens,predicted_tokens)\n",
        "    return format_albert_tokens_as_string(replaced_tokens)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "facde541435241d2a630c11146083d41",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eed8c7292404ee78458823ee54221d3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFKjhqbXm0k8"
      },
      "source": [
        "Replacing the masked token is simply a matter of finding the predicted token in the position of the masked token\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbZK05vim19K"
      },
      "source": [
        "def replace_mask_tokens_with_predicted_tokens(tokens:List[str],predictions:List[str]) -> List[str]:\n",
        "\tfor index,token in enumerate(tokens):\n",
        "\t\tif token==mask_token:\n",
        "\t\t\ttokens[index] = predictions[index]\n",
        "\treturn tokens\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1fTvUAZnH0I"
      },
      "source": [
        "And the tokens (with the masks replaced with predictions) are then converted into a string - cutting off the special tokens added by albert's tokeniser to the start (\"[CLS]\") and end (\"[SEP]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jxAeXkOnGiD"
      },
      "source": [
        "def format_albert_tokens_as_string(tokens:List[str]) -> str:\n",
        "    return ''.join(map(lambda token:token.replace(\"▁\",\" \"), tokens[1:-1]))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "72718ZmAnc0n",
        "outputId": "aa13520f-9313-4281-f37e-ca76cd6442ab"
      },
      "source": [
        "predict_masked_words(\"i went to the [MASK] the other day\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' i went to the dentist the other day'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "beo0UArMni9j",
        "outputId": "f729c412-99d9-46cb-e9af-7fbad78fc138"
      },
      "source": [
        "grow_question(\"why cats?\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' but why cats?'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9fk2iynnq49"
      },
      "source": [
        "We can repeat this process as many times as you like to grow the question iteratively\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDrppCNRnswF"
      },
      "source": [
        "def generate(text:str, max_iterations:int=5) -> str:\n",
        "\tquestion = extract_seed_question(text)\n",
        "\tfor iteration in range(max_iterations):\n",
        "\t\tquestion = grow_question(question)\n",
        "\treturn question\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xddGehArnxyT",
        "outputId": "dc17930e-bb93-4306-f4c0-df9f3ca262e7"
      },
      "source": [
        "generate(text=\"i like cats\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' what sounds like feral feral feral cats? ?'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc4yMsemo9je"
      },
      "source": [
        "def remove_tokens_for_fluency(text:str,tokens:List[str]) -> str:\n",
        "  for token in tokens:\n",
        "    text = text.replace(token,'')\n",
        "  return text\n",
        "\n",
        "ignored_albert_tokens = [\"evalle\",\"joyah\"]\n",
        "\n",
        "def generate(text:str, max_iterations:int=5) -> str:\n",
        "  question = extract_seed_question(text)\n",
        "  for iteration in range(max_iterations):\n",
        "    question = grow_question(question)\n",
        "  question = remove_tokens_for_fluency(question,ignored_albert_tokens)\n",
        "  question = rephrase_for_fluency(question) \n",
        "  return question"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JUmsPdEYpi5e",
        "outputId": "7c136c8f-2dda-4d10-bfe1-5b12d0171356"
      },
      "source": [
        "generate(text=\"i like cats\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'When do I like to feed my cat stuffed?'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8-lfD-9oN4f"
      },
      "source": [
        "**Pros**:\n",
        "\n",
        "- The model works off-the-shelf without needing to be retrained or fine-tuned \n",
        "\n",
        "- The generated questions add far more novelty and engagement than the baseline, while keeping it relevant (e.g. \"Are you wondering what cats really look like?\" and \"Could you love two of your favorite stuffed cats?\" as questions to the utterance \"i like cats\")\n",
        "\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "- The question forms are constrained to the typical Wh-questions (e.g. \"who..?\", \"what...?\", \"where..?\", etc) and cannot considerably vary from these formats for additional surprise and creativity often found in natural questions (e.g. \"i always wanted furry cats but I hear they require a lot of grooming dont they?\")\n",
        "\n",
        "- The generated questions are not always perfect and sometimes require post-processing (i.e. removing certain spurious tokens, spinning for fluency, etc)\n",
        "\n",
        "- The seed question still depends on the baseline's heuristics to extract a couple of keywords from the utterance  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "LWQP003poOh5",
        "outputId": "9bf74b97-be68-48fc-c426-c300d4a4f23a"
      },
      "source": [
        "while True:\n",
        "  print(generate(input(\">\")))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">this is a test\n",
            "Why the test?\n",
            ">i just want to check if you work\n",
            "What does check work?\n",
            ">i like cats\n",
            "Do you like to eat these stuffed cats?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-43429122f431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADRqRsetkmFI"
      },
      "source": [
        "# 3. Semantic-Image Captioning Character-based LSTM (Unsupervised Learning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "834SO_kmps1i"
      },
      "source": [
        "In order to add even more originality to our generated questions, we are going to have to start training a model. However, this can often lead to another problem for ML tasks, finding labelled data for your specific task (i.e. utterances -> question). If there are no datasets publically available, it can be a very expensive to label a new dataset from scratch!  Fortunately, for this particular task, it so happens that there is a way we can use unlabelled data to train a model to generate questions given utterances!\n",
        "\n",
        "The problem can be thought of as encoding the utterances into a semantic space and then decoding these semantics into a question (utterance -> semantics -> question). \n",
        "\n",
        "Since there are already a plethora of pre-trained methods readily available for embedding utterances into a semantic space (e.g. `word2vec`, `universal sentence encoder`, etc), we can leverage this fact to simplify our model's learning task. We simply need to train a decoder (a model that converts the semantics -> question) since the encoder (The model which converts the utterance -> semantics) is already solved.  We are essentially learning to decode the semantic vector of a pre-trained sentence encoder (and in so doing, cutting our model's learning objective in half). In other words, we are creating an `encoder-decoder` model by fitting a decoder (e.g. a `character-based LSTM`) to a pre-trained encoder (e.g. `spacy`). \n",
        "\n",
        "This approach is common for Image-Captioning tasks (whereby a decoder is trained to convert an image vector into a caption and the image vector is often obtained via a pre-trained image encoder, such as `Resnet`, etc).  Our formulation of the task, therefore, would be a form of `Semantic-Captioning`.\n",
        "\n",
        "**But how does this allow our model to use unlabelled data?**  Here comes the secret sauce: \n",
        "\n",
        "Since the semantics of the utterances are formless (i.e. they are no longer dependent on how the words are phrased, etc) we can obtain the semantics from utterances which are formulated as questions too (e.g. question -> semantics -> question) - as the semantics of an utterance, phrased as a sentence or not, will look near identical in the semantic space!  Knowing this, we can simply use a large list of unlabelled questions for both the expected outputs (that the decoder must learn to generate) and their semantics can also act as the training inputs. The effect of this will be that the decoder learns to generate questions to any utterances with similar semantics to the question itself (ensuring the question is relevant and coherent to the utterances given). This is how we shall generate a question given the semantics of an utterance similar to that question. Thus we are actually doing `unsupervised learning` using only a set of unlabelled questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7dVZbHjp2HW"
      },
      "source": [
        "**Model**:\n",
        "\n",
        "We build our character-based LSTM decoder in keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8RKnkRp2wY"
      },
      "source": [
        "from keras import Model\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, Embedding, add\n",
        "\n",
        "def build_character_based_LSTM(\n",
        "    semantic_vector_length:int,\n",
        "    character_vector_length:int,\n",
        "    character_sequence_length:int,\n",
        "    hidden_layer_length:int,\n",
        "    dropout_rate:float,\n",
        "    optimisation:str,\n",
        "    activation:str,\n",
        "    weights:Optional[str],\n",
        "    loss:str,\n",
        ") -> Model:\n",
        "        meaning_layer1 = Input(shape=(semantic_vector_length,))\n",
        "        meaning_dropout1 = Dropout(dropout_rate)(meaning_layer1)\n",
        "        meaning_layer2 = Dense(hidden_layer_length, activation=activation)(meaning_dropout1)\n",
        "        characters_layer1 = Input(shape=(character_sequence_length,))\n",
        "        characters_layer2 = Embedding(character_vector_length, character_vector_length, mask_zero=True)(characters_layer1)\n",
        "        characters_dropout2 = Dropout(dropout_rate)(characters_layer2)\n",
        "        characters_layer3 = LSTM(hidden_layer_length)(characters_dropout2)\n",
        "        layer3 = add([meaning_layer2, characters_layer3])\n",
        "        layer4 = Dense(hidden_layer_length, activation=activation)(layer3)\n",
        "        layer5 = Dense(character_vector_length, activation='softmax')(layer4)\n",
        "        model = Model(\n",
        "            inputs=[meaning_layer1, characters_layer1], \n",
        "            outputs=layer5\n",
        "        )\n",
        "        if weights is not None: model.load_weights(weights)\n",
        "        model.compile(loss=loss, optimizer=optimisation)\n",
        "        return model \n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivS2PcZBqJtq"
      },
      "source": [
        "To keep the overall model light, we use spacy as our pre-trained encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVhLfXB8qLJB"
      },
      "source": [
        "from numpy import array\n",
        "from spacy import load \n",
        "\n",
        "semantic_encoder = load('en_core_web_sm')\n",
        "\n",
        "def _get_semantic_vector(text:str) -> array:\n",
        "  if not any(text):text=start_token\n",
        "  return semantic_encoder(text).vector\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yzB1aB0qQty",
        "outputId": "750845eb-978a-494a-b79a-db599d33cb45"
      },
      "source": [
        "_get_semantic_vector(\"example\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.16552697, -1.6355336 , -0.82578963, -1.5218631 ,  0.578955  ,\n",
              "        1.0864875 ,  3.0055983 , -0.7495016 ,  0.76283836,  1.8960434 ,\n",
              "        3.2863004 ,  0.658136  ,  0.5691658 , -0.9015105 , -1.1981287 ,\n",
              "        1.0211538 , -0.01941079,  1.3651531 , -2.3700018 ,  0.15563929,\n",
              "        0.31348708,  1.8830317 , -0.01520303, -1.5158546 , -0.31715876,\n",
              "       -1.3962522 , -0.48482934, -1.3297229 ,  2.729463  ,  0.9482241 ,\n",
              "        3.821124  ,  2.2941923 , -0.3093573 ,  1.6531448 ,  1.1451837 ,\n",
              "       -2.2079291 ,  0.5225949 ,  0.02467287,  0.16249415,  1.0908791 ,\n",
              "        4.359484  , -0.53804994, -1.1284481 , -2.38859   ,  0.73491585,\n",
              "       -0.8134295 , -1.0205336 ,  0.23044568, -1.9584979 ,  2.5816565 ,\n",
              "       -0.9460479 , -2.2546642 , -1.344979  , -1.342088  , -0.64584696,\n",
              "        0.76589775,  2.251912  ,  0.2361167 ,  0.7278886 ,  1.7257209 ,\n",
              "        2.016412  , -1.1067169 , -0.7374257 , -0.26853034,  0.8136801 ,\n",
              "       -3.8449912 ,  1.905324  , -3.5445404 , -2.90351   , -1.1220598 ,\n",
              "       -3.6228151 ,  0.15611175,  2.2736704 ,  1.8579361 , -2.3048258 ,\n",
              "       -1.3726324 ,  2.3060074 , -0.97374785, -2.3150702 ,  3.7444696 ,\n",
              "       -0.6814463 , -2.168365  , -0.62721217, -0.5490932 ,  2.394304  ,\n",
              "        3.5844135 , -0.46525475, -1.8458382 , -2.0020945 , -2.5614429 ,\n",
              "        1.15705   ,  0.75333565, -1.8163193 ,  5.28741   ,  0.42514443,\n",
              "       -1.1579907 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6qWUckzqVC9"
      },
      "source": [
        "The output characters which the decoder model will be able to generate are the letters between a-z, a space and two special tokens to indicate the start and end of a question:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy1qIw0UqWs4",
        "outputId": "5af7f828-31c3-4523-d809-4feeffa8ddaa"
      },
      "source": [
        "a = ord('a')\n",
        "z = ord('z')\n",
        "start_token='|'\n",
        "stop_token='?'\n",
        "character_set = [' '] + list(map(chr,range(a,z+1))) + [start_token,stop_token]\n",
        "number_of_characters = len(character_set)\n",
        "character_set"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmWmC8_Mqc6G"
      },
      "source": [
        "The maximum recursion depth of our LSTM will determine the maximum length of the generated question (e.g. 100 characters). The other hyperparameters of our model are specified below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF7wa8xdqcax"
      },
      "source": [
        "recursion_depth = 100\n",
        "path_to_model_weights=None\n",
        "\n",
        "model = build_character_based_LSTM(\n",
        "    semantic_vector_length=len(_get_semantic_vector(\"\")),\n",
        "    character_vector_length=number_of_characters,\n",
        "    character_sequence_length=recursion_depth,\n",
        "    hidden_layer_length=256,\n",
        "    dropout_rate=.5,\n",
        "    activation=\"relu\",\n",
        "    loss = \"categorical_crossentropy\",\n",
        "    optimisation= \"adam\",\n",
        "    weights=path_to_model_weights,\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5EC0yNvqDyE",
        "outputId": "8664f7a7-b4ee-4201-b89d-6715bbd697bf"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 100, 29)      841         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 96)           0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100, 29)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          24832       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          292864      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 29)           7453        dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 391,782\n",
            "Trainable params: 391,782\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "n-Pk9_8feTH7",
        "outputId": "dca9ecae-1483-4754-babf-69f4c9344819"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S5jYk6IrLYj"
      },
      "source": [
        "To generate a question, we iteratively predict the next character until we reach the maximum recursion_depth or we encounter the special token that indicates the end of a question\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "294WXfpQrNJ8"
      },
      "source": [
        "def generate(sentence:str) -> str:\n",
        "    generated_question = start_token\n",
        "    for _ in range(recursion_depth):\n",
        "        generated_question += _predict_next_character(\n",
        "            meaning=sentence,\n",
        "            contextual_characters=generated_question,\n",
        "        )\n",
        "        if stop_token in generated_question:\n",
        "            break\n",
        "    return generated_question"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL-ebWUFrSI7"
      },
      "source": [
        "\n",
        "Whereby to predict the next character in the sequence, we input the semantic vector of the utterance and the generated sequence of characters so far as inputs into our trained LSTM.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc-nnlcLrTn0"
      },
      "source": [
        "def _predict_next_character(meaning:str,contextual_characters:str) -> str:\n",
        "  X_meaning = array([_get_semantic_vector(meaning)])\n",
        "  X_characters = array([_pad(_convert_characters_to_index(contextual_characters))])\n",
        "  output_vector = model.predict((X_meaning,X_characters),verbose=False)\n",
        "  predicted_index = _greedy_decode(output_vector)\n",
        "  return index_character_mapping.get(predicted_index)\n",
        "    "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acl0A32vreZt"
      },
      "source": [
        "Note that while the input utterance semantics are represented by a fixed-length vector (the output of the pre-trained encoder) the input sequence generated so far must also be converted into a fixed-length vector (achieved by padding the sequence with 0s if it is shorter than the full sequence length - which is the maximum recursion_depth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5RZGBlbrf79"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def _pad(indexes:List[int]) -> array:\n",
        "    return pad_sequences([indexes], maxlen=recursion_depth)[0]\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_j8JCpgrhGX",
        "outputId": "47514f41-35ac-430f-a3fb-5c9d921d9fa5"
      },
      "source": [
        "_pad([1,2,3])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ZMZ_-drm1t"
      },
      "source": [
        "Also note that the model expects the character indexes (as opposed to the characters as strings) and also outputs character indexes as predictions.  Therefore, we need to dictionaries to map characters into indexes (`character_index_mapping`) and indexes back into characters again (`index_character_mapping`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6zn62TvroeM"
      },
      "source": [
        "index_character_mapping = dict(enumerate(character_set))\n",
        "\n",
        "character_index_mapping = {\n",
        "    character:index for index,character in index_character_mapping.items()\n",
        "}\n",
        "\n",
        "def _convert_characters_to_index(characters:str) -> List[int]:\n",
        "    return list(map(character_index_mapping.get,characters))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLBh1j3Jrp9O",
        "outputId": "af7987ca-6ed6-4fb3-e29d-43a626f3b2f2"
      },
      "source": [
        "_convert_characters_to_index(\"example\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 24, 1, 13, 16, 12, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU8vjDwurySt"
      },
      "source": [
        "\n",
        "**Decoding Strategies**:\n",
        "\n",
        "One of the most important factors about NLG (if not The Most Important) is the decoding strategy.  It is a hot topic of research!\n",
        "\n",
        ">\"Neural probabilistic text generators are far from\n",
        "perfect; prior work has shown that they often generate text that is generic, unnatural and sometimes even non-existent\". \n",
        "\n",
        "The model's outputs actually predict something akin to a probability distribution across the k possible output characters.  A `greedy` decoding strategy would simply take whichever index has the largest value (`argmax`) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUoUsbo0rz85"
      },
      "source": [
        "def _greedy_decode(predicted_vector:array) -> int:\n",
        "    return predicted_vector[0].argmax()\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX92WCLqr4Yc",
        "outputId": "0c7f9100-15a6-4b04-c0f0-590222c1f32d"
      },
      "source": [
        "y_hat = array([[.005, .124, .1, .13, .16, .12, .05]])\n",
        "_greedy_decode(y_hat)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1ms1PnaprYkh",
        "outputId": "bcb26e68-01be-40eb-df27-518d78456c6f"
      },
      "source": [
        "_predict_next_character(\"this is an example\", \"|\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'k'"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1zBDRXH3rPDW",
        "outputId": "6e6dc73c-28ef-497e-b02a-9e5015e33635"
      },
      "source": [
        "generate(\"this is an example\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk'"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax52ILzKsTl2"
      },
      "source": [
        "However, this will not yield the best results in the long-run.  Even thought we greedily select the next most likely character (the `local optimum`) it does not mean this character leads to a sequence of characters which have the highest probability overall (the `global optimum`). \n",
        "\n",
        "For example, the token \"park\" may have a higher probability (.36) than the token \"grocery\" (.15), but only when you pick \"grocery\" can you have the sequence \"grocery store\" which has an overall higher probability score (.135) than any sequence that could be created with the token \"park\" (e.g. \"park today\" has a lower probability of .12)\n",
        "\n",
        "A simple alternative, then, is to sample from the entire output according to their probabilities (so that the characters with higher probabilities are more likely to be selected) but still allowing the chance for less likely characters to be selected too.  While this random sampling strategy works better than the greedy strategy (which actually only produces a sequence of \" \" characters in both examples below), it can lead to some fairly noisy results.\n",
        "\n",
        "Therefore we can introduce a constant called `temperature` to multiply with the probabilities as a way to increase the probability of picking the more likely characters and decrease the probability of picking the least likely characters (which are the source of the noise).  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LevLLyesYZc"
      },
      "source": [
        "from numpy.random import choice\n",
        "\n",
        "def _temperature_decode(predicted_vector:array,temperature:float) -> int:\n",
        "    probabilities = predicted_vector[0]\n",
        "    probabilities *= temperature\n",
        "    probabilities /= probabilities.sum()\n",
        "    return choice(range(probabilities.size),1,p=probabilities)[0]\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVg2mqcjsaW5",
        "outputId": "b9ded3e3-119a-41b6-9fbc-9e1aa1492a4a"
      },
      "source": [
        "temperature = .5\n",
        "_temperature_decode(y_hat,temperature)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqCo5luestVE",
        "outputId": "aae22ede-157d-49ff-9406-52a569c8f663"
      },
      "source": [
        "_temperature_decode(y_hat,temperature)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFJz9uR2suTu",
        "outputId": "4391f635-53ab-4ca3-a144-aec02fab0f5f"
      },
      "source": [
        "_temperature_decode(y_hat,temperature)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5liHQ_5Nv0s4"
      },
      "source": [
        "def _predict_next_character(meaning:str,contextual_characters:str) -> str:\n",
        "  X_meaning = array([_get_semantic_vector(meaning)])\n",
        "  X_characters = array([_pad(_convert_characters_to_index(contextual_characters))])\n",
        "  output_vector = model.predict((X_meaning,X_characters),verbose=False)\n",
        "  predicted_index = _temperature_decode(output_vector,.5)\n",
        "  return index_character_mapping.get(predicted_index)\n",
        "    "
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F668aU7Fv7rn",
        "outputId": "dc74625d-850f-4d7a-ba02-241949d6b05e"
      },
      "source": [
        "generate(\"this is an example\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|  dyse ien   iddrt   r rc t n pdp  y psip ooc p cpii    rrtrtit tilt?'"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrR1ke4dsjH4"
      },
      "source": [
        "This produces better generated sequences of characters than greedy with far less noise than pure random sampling\n",
        "\n",
        "Perhaps one-step better than this is `nucleus sampling` (p-decoding) wherein only the indexes which make up the top_p % of the probability distribution are sampled from. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pO9Rdn1sl0H"
      },
      "source": [
        "def _nucleus_decode(predicted_vector:array, top_p:float, temperature:Optional[float]) -> int:\n",
        "    probabilities = predicted_vector[0]\n",
        "    probabilities /= probabilities.sum()\n",
        "    ranked_indexes = sorted(range(probabilities.size), key=lambda index:probabilities[index],reverse=True)\n",
        "    probabilities.sort()\n",
        "    ranked_probabilities = probabilities[::-1]\n",
        "    for index in range(ranked_probabilities.size):\n",
        "        if ranked_probabilities[:index].sum() >= top_p:\n",
        "            nuclues_probabilities = ranked_probabilities[:index]\n",
        "            nuclues_indexes = ranked_indexes[:index]\n",
        "            break\n",
        "    if temperature: nuclues_probabilities *= temperature\n",
        "    nuclues_probabilities /= nuclues_probabilities.sum()\n",
        "    return choice(nuclues_indexes,1,p=nuclues_probabilities)[0]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jiHxd1hsm5M",
        "outputId": "a2dbe86e-ac9f-4d28-e248-d25a7fe6a42e"
      },
      "source": [
        "top_p = .8\n",
        "_nucleus_decode(y_hat, top_p, temperature)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTof4WNEsv6t",
        "outputId": "98c20fe7-b440-4a92-aad9-6dc02dab1331"
      },
      "source": [
        "_nucleus_decode(y_hat, top_p, temperature)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoTd4asjswv9",
        "outputId": "9327b45f-9f54-4c90-e998-b79beac7b36d"
      },
      "source": [
        "_nucleus_decode(y_hat, top_p, temperature)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUlhCtEBwABY"
      },
      "source": [
        "def _predict_next_character(meaning:str,contextual_characters:str) -> str:\n",
        "  X_meaning = array([_get_semantic_vector(meaning)])\n",
        "  X_characters = array([_pad(_convert_characters_to_index(contextual_characters))])\n",
        "  output_vector = model.predict((X_meaning,X_characters),verbose=False)\n",
        "  predicted_index = _nucleus_decode(output_vector,.8, .5)\n",
        "  return index_character_mapping.get(predicted_index)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4B_w8hQywEgn",
        "outputId": "880df89a-65fb-4811-ee45-7254fc6dd4ec"
      },
      "source": [
        "generate(\"this is an example\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|  ann enid e  nedteirnsr  pninppppcp pc pciiei is ktticitt?'"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G7GgXCbsyzf"
      },
      "source": [
        "Nucleus sampling produces slightly higher quality text as compared with temperature sampling.  However, we can also include temperature with nucleus sampling to improve it even further: \n",
        "\n",
        "\n",
        "However, for near optimal results, it has become the industry standard to use `beam search` (maintaining the top-k generated sequences).  However, maintaining multiple sequences (even if constrained to k beams) is much more computationally expensive than keeping track of a single sequence (as the above strategies do).  Is there any way to arrive at a near-optimal sequence without having to maintain multiple sequences (as with beam search)?  \n",
        "\n",
        "Beam search, however, is somewhat of an enigma.  It somehow manages to generate near-optimal sequences with only a small number of beams.  What is more mysterious, however, is the the phenomena known as the `beam search curse` \n",
        "\n",
        "> \"a specific phenomenon where using a larger beam\n",
        "size leads to worse performance\"\n",
        "\n",
        "In fact, \"the success of beam search does not stem from its ability to approximate exact decoding in practice, but rather due to a hidden inductive bias embedded in the algorithm. This inductive bias appears to be paramount for generating desirable text\". This [paper](https://aclanthology.org/2020.emnlp-main.170.pdf) analyses exactly how beam search is biasing generated sequences, in the hope that understanding this will allow for a more direct way to replicate the success of a beam search. The authors conclude that: \n",
        "\n",
        "> \"We provide a plausible answer—inspired by psycholinguistic theory—as to why beam search (with small beams) leads to high-quality text\" \n",
        "\n",
        ">\"beam search enforces uniform information density in text\"\n",
        "\n",
        ">\"beam search is trying to optimize for UID\"\n",
        "\n",
        ">\"beam search has an inductive bias which can be linked to the promotion of uniform information density (UID), a theory from cognitive science regarding even distribution of information in linguistic signals\"\n",
        ">> \"The UID hypothesis states that—subject to the constraints of the grammar—humans prefer sentences that distribute information (in the sense of information theory) equally across the linguistic signal, e.g., a sentence. In other words, human-produced text, regardless of language, tends to have evenly distributed surprisal, formally defined in information theory as negative log-probability\"\n",
        "\n",
        "They then suggest \"a battery of possible sentence-level UID measures\" as candidate \"decoding objectives that explicitly enforce this property\" to replicate the success of beam search (note the above example depicting one such objective on a single sequence which generates the exact same sequence as generated by a k=5 beam search)\n",
        "\n",
        ">\"This insight naturally leads to the development of several new regularizers that likewise enforce the UID property\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCqBvsBvs_qH"
      },
      "source": [
        "**Training**:\n",
        "\n",
        "Training the model is as simple as fitting it to the formatted question data (We also save the model weights after each epoch so that we can load in the trained model later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUBNEmXss92d"
      },
      "source": [
        "def train(\n",
        "    questions:List[str], \n",
        "    batch_size:int, epochs:int, \n",
        "    save_to_file_path:str=\"char_lstm_weights.hdf5\",\n",
        ") -> None:\n",
        "    steps_per_epoch = len(questions)//batch_size\n",
        "    for epoch in range(epochs):\n",
        "        model.fit(\n",
        "            _get_training_data(questions, batch_size), \n",
        "            verbose=True, epochs=1, steps_per_epoch=steps_per_epoch, \n",
        "        )\n",
        "        model.save_weights(save_to_file_path)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AixLhDA8tFHp"
      },
      "source": [
        "To format the training data, we create a generator to iterate through the questions and get its semantics, then we convert the characters into indexes and iterate through them one by one, making each an expected character output (encoded as a one-hot vector) with the preceding characters the training inputs (padded to ensure consistent sequence lengths).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz7BDr1wtEOb"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "def _one_hot_encode_output(index:int) -> array:\n",
        "    return to_categorical([index], num_classes=number_of_characters)[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykn5PYAXtHz1",
        "outputId": "f784467d-4c1e-4905-b7a2-be124d85439f"
      },
      "source": [
        "_one_hot_encode_output(4)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT-MFX7stMxH"
      },
      "source": [
        "When a batch has been reached, the semantic vectors (`X_meaning`) and the input character sequences (`X_characters`) are yielded along with the expected character outputs (`Y_characters`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KkpG0wFtOhN"
      },
      "source": [
        "def _get_training_data(questions:List[str],batch_size:int) -> Tuple[Tuple[array,array],array]:\n",
        "    iteration_count,X_meaning, X_characters, Y_characters = 0,[],[],[]\n",
        "    while True:\n",
        "        for question in questions:\n",
        "            iteration_count+=1\n",
        "\n",
        "            semantic_vector = _get_semantic_vector(question)\n",
        "            processed_question = _preprocess_characters(question)\n",
        "            character_indexes = _convert_characters_to_index(processed_question)\n",
        "            for index in range(1, len(character_indexes)):\n",
        "                contextual_character_indexes = character_indexes[:index]\n",
        "                next_character_index = character_indexes[index]                    \n",
        "                X_meaning.append(semantic_vector)\n",
        "                X_characters.append(_pad(indexes=contextual_character_indexes))\n",
        "                Y_characters.append(_one_hot_encode_output(next_character_index))\n",
        "\n",
        "            if iteration_count==batch_size:\n",
        "                yield ([array(X_meaning), array(X_characters)],array(Y_characters))\n",
        "                iteration_count,X_meaning, X_characters, Y_characters = 0,[],[],[]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGQ-XKpstgL_"
      },
      "source": [
        "clean_string = lambda text:''.join(filter(is_valid_character,text.lower().strip()))\n",
        "is_valid_character = lambda character: character.isspace() or a <= ord(character) <= z\n",
        "\n",
        "def _preprocess_characters(text:str) -> str:\n",
        "    return f\"{start_token} {clean_string(text)} {stop_token}\"\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1kMEJhRt-iB"
      },
      "source": [
        "**Dataset**:\n",
        "\n",
        "Now we can train our decoder model, we just need a dataset of unlabelled questions.  We can pick one of the many datasets which contain questions, such as the `Quora` question pairs dataset. \n",
        "\n",
        "The dataset is publically available and can be downloaded from [here](https://raw.githubusercontent.com/MLDroid/quora_duplicate_challenge/master/data/quora_duplicate_questions.tsv)\n",
        "\n",
        "Since we dont require labelled questions, we will just extract all the unique questions in the dataset and ignore any labels they provide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gib6l0gUuDRD",
        "outputId": "3fc62e59-708e-4491-f464-6e16de7616fa"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MLDroid/quora_duplicate_challenge/master/data/quora_duplicate_questions.tsv"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-13 01:15:29--  https://raw.githubusercontent.com/MLDroid/quora_duplicate_challenge/master/data/quora_duplicate_questions.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58176133 (55M) [text/plain]\n",
            "Saving to: ‘quora_duplicate_questions.tsv’\n",
            "\n",
            "quora_duplicate_que 100%[===================>]  55.48M   159MB/s    in 0.3s    \n",
            "\n",
            "2021-09-13 01:15:30 (159 MB/s) - ‘quora_duplicate_questions.tsv’ saved [58176133/58176133]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSNJ9Pkot_cz"
      },
      "source": [
        "from pandas import read_csv\n",
        "\n",
        "def load_quora_data(path_to_file:str) -> List[str]:\n",
        "    quora_dataset = read_csv(path_to_file, sep='\\t', header=0)\n",
        "    questions = quora_dataset.question1.append(quora_dataset.question2)\n",
        "    questions = questions.apply(str)\n",
        "    return sorted(set(questions))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY6i01oMuU9W",
        "outputId": "79fb2dbb-a458-45a8-88e3-314b77fbc49d"
      },
      "source": [
        "first_ten_questions_only = load_quora_data(\"quora_duplicate_questions.tsv\")[:10]\n",
        "first_ten_questions_only"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Any Canadian teachers (B.Ed. holders) teaching in U.S. schools?',\n",
              " ' Are there any underlying psychological tricks/tactics that are used when designing the lines for rides at amusement parks?',\n",
              " ' Can I pay with a debit card on PayPal?',\n",
              " ' Does New York state have a flagship university?',\n",
              " ' Failures haunt me all the time.How do I cope up?',\n",
              " ' How can I improve my sex life?',\n",
              " ' How do I make the time lapse images using an EOS 70D, with an intervalometer?',\n",
              " ' How will you interpret my dream?',\n",
              " \" I am a 5 letter word.  I am normally below u  If u remove my 1st letter   u'll find me above u  If u remove my 1st & 2nd letters  u cant see me  Answer is really very interesting  Let us see who solves this.... ⏰Time limit :- today U can also send to other grps if I?\",\n",
              " \" I didn't file a police report for a car accident that happened over a month ago. My insurance company won't pay for damage, what do I do?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRnsNC_JughR",
        "outputId": "dfebcb8d-ac77-4227-aea3-6407a1de30d8"
      },
      "source": [
        "train(first_ten_questions_only,batch_size=6,epochs=5,save_to_file_path=\"lstm_quora.hdf5\") \n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 607ms/step - loss: 1.9600\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 1.9850\n",
            "1/1 [==============================] - 1s 591ms/step - loss: 1.9920\n",
            "1/1 [==============================] - 1s 612ms/step - loss: 1.9910\n",
            "1/1 [==============================] - 1s 630ms/step - loss: 1.9355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alPjHNAuvNFD",
        "outputId": "b6186b19-bcae-4d8a-91d4-7c9c6cd2feb3"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm_quora.hdf5  model.png  quora_duplicate_questions.tsv  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GdO8_9o-vOTX",
        "outputId": "47519e3f-8a97-4f00-9bcc-4305ff2a72fc"
      },
      "source": [
        "generate(\"this is an example\")"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|yi r   e     i  nn e innnen  ne   st cn ippplplag cc  pcepnpitkttctc?'"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVhSF8uWwT9M"
      },
      "source": [
        "**Pros**:\n",
        "\n",
        "- Only requires unlabelled data to train (unsupervised learning)\n",
        "\n",
        "- Since the utterances are vectorised into a fixed-length semantic vector, the input length is not a problem (it can be any length - be it an entire conversation or a single word)!\n",
        "\n",
        "- since we are only training a decoder, it is far lighter than learning an entire encoder-decoder model \n",
        "\n",
        "- Since the decoder is a character-level LSTM, it has a very small output size (as the vocabulary consists of just the alphabet)\n",
        "\n",
        "- The model is able to generate more creative, natural sounding questions (which need not start with typical \"Wh\"-question words)\n",
        "\n",
        "**Cons**:\n",
        "\n",
        "- The encoder is very simple and a far superior semantic representation can be achieved if this is replaced\n",
        "\n",
        "- The decoder could be trained in less iterations and produce more exressive questions still if we were to fine-tune a pre-trained generative language model\n",
        "\n",
        "- The decoding strategy is currently temperature sampling and could be improved using nucleus-sampling, beam-search or one of the suggested uid measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIkbteowawM"
      },
      "source": [
        "# 4. Semantic-Image Captioning Character-based LSTM (Supervised Learning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2O5fHENwclr"
      },
      "source": [
        "Although the model described above can be trained on unlabelled questions, it can also be trained on labelled data in a more typical supervised fashion, if such data is available! When training the model, we would just use the semantics of the utterances (instead of the question) and everything else remains the same as before.\n",
        "\n",
        "This method can, in theory, produce a better trained model than the unsupervised version simply because you can train it to respond with questions that have very different semantics than the input utterances (to steer the conversation into a new direction or add a bit more novelty). \n",
        "\n",
        "It so happens that for this task, there is an abundance of datasets to map utterances to questions!  Its just that we need to be a bit creative in how we view these datasets.  For instance, every single question-answer dataset (e.g. `SQUAD2.0`, `Google Natural Questions`, etc) is mapping a question to an answer.  So if we reverse the order, we get a dataset that maps answers (or utterances) onto questions! Et Voila!\n",
        "\n",
        "Lets show how this is done with SQUAD2.0 which we can download from [here](https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json)\n",
        "\n",
        "The SQUAD dataset comes as a json with the following keys: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBJUdfDRwn6a",
        "outputId": "aa181a4f-024b-4408-8aa9-63355ab3c347"
      },
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-13 01:25:42--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M   140MB/s    in 0.3s    \n",
            "\n",
            "2021-09-13 01:25:43 (140 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R08Okltwexs"
      },
      "source": [
        "from enum import Enum\n",
        "\n",
        "class SquadDataKeys(Enum):\n",
        "    DATA = \"data\"\n",
        "    TITLE = \"title\"\n",
        "    PARAGRAPH = \"paragraphs\"\n",
        "    CONTEXT = \"context\"\n",
        "    QUESTIONANSWER = \"qas\"\n",
        "    QUESTION = \"question\""
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbK8c-Gxwg3x"
      },
      "source": [
        "We can load in the json and iterate through it to extract the titles (as our short utterances) and the contexts (as our long utterances) and the questions that go with them:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMpLvpXTwgXe"
      },
      "source": [
        "from json import loads\n",
        "\n",
        "def load_squad_data(path_to_file:str) -> List[Tuple[str,str]]:\n",
        "    with open(path_to_file) as json_file:\n",
        "        squad2_data = loads(json_file.read())\n",
        "        for topic_data in squad2_data[SquadDataKeys.DATA.value]:\n",
        "            short_context = topic_data[SquadDataKeys.TITLE.value]\n",
        "            for paragraph in topic_data[SquadDataKeys.PARAGRAPH.value]:\n",
        "                long_context = paragraph[SquadDataKeys.CONTEXT.value]\n",
        "                for index,question_data in enumerate(paragraph[SquadDataKeys.QUESTIONANSWER.value]):\n",
        "                    question = question_data[SquadDataKeys.QUESTION.value]\n",
        "                    if index == 0: yield (short_context, question)\n",
        "                    yield (long_context, question)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtnoEJqMwkNR"
      },
      "source": [
        "contexts,questions = zip(*load_squad_data(\"train-v2.0.json\"))\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU-BaAaKwuzW",
        "outputId": "f689b140-7191-4be4-9b14-1ee75df49a9d"
      },
      "source": [
        "contexts[:4]"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Beyoncé',\n",
              " 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".')"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMulj3FrwxQs",
        "outputId": "7fcd0aab-8a3b-4b16-a702-6ad0b3a58b79"
      },
      "source": [
        "questions[:4]\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('When did Beyonce start becoming popular?',\n",
              " 'When did Beyonce start becoming popular?',\n",
              " 'What areas did Beyonce compete in when she was growing up?',\n",
              " \"When did Beyonce leave Destiny's Child and become a solo singer?\")"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7oqazzrxaHm"
      },
      "source": [
        "It also happens we have an even better dataset which was actually designed to provides utterances and then questions (well actually it was designed to simulate realistic conversations, but it so happens to have questions within the conversations).  This dataset is `DailyDialog` which we can download from [here](https://aclanthology.org/attachments/I17-1099.Datasets.zip)\n",
        "\n",
        "This dataset also provides dialogue_act labels which we can use to identify the questions automatically and extract them along with the the utterances that precede it to form our training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAS38V5Hxm1u",
        "outputId": "4043f1e0-bcb2-46aa-bf2a-ac170a3fd422"
      },
      "source": [
        "!wget https://aclanthology.org/attachments/I17-1099.Datasets.zip"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-13 01:30:06--  https://aclanthology.org/attachments/I17-1099.Datasets.zip\n",
            "Resolving aclanthology.org (aclanthology.org)... 174.138.37.75\n",
            "Connecting to aclanthology.org (aclanthology.org)|174.138.37.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4751210 (4.5M) [application/zip]\n",
            "Saving to: ‘I17-1099.Datasets.zip’\n",
            "\n",
            "I17-1099.Datasets.z 100%[===================>]   4.53M  18.7MB/s    in 0.2s    \n",
            "\n",
            "2021-09-13 01:30:06 (18.7 MB/s) - ‘I17-1099.Datasets.zip’ saved [4751210/4751210]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kgrJ-eIxqm9",
        "outputId": "15f8e539-924c-4d88-950b-426cec31c5de"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I17-1099.Datasets.zip  model.png\t\t      sample_data\n",
            "lstm_quora.hdf5        quora_duplicate_questions.tsv  train-v2.0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaPFNul5x6WL",
        "outputId": "d395a56a-cf17-407c-8d21-baeb880089de"
      },
      "source": [
        "!sudo apt-get install unzip\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4GI0dpOx-IZ",
        "outputId": "fc58c761-58a9-4c78-b7a1-8b0f07796b26"
      },
      "source": [
        "!unzip I17-1099.Datasets.zip"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  I17-1099.Datasets.zip\n",
            "   creating: EMNLP_dataset/\n",
            "  inflating: EMNLP_dataset/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/EMNLP_dataset/\n",
            "  inflating: __MACOSX/EMNLP_dataset/._.DS_Store  \n",
            "  inflating: EMNLP_dataset/dialogues_act.txt  \n",
            "  inflating: EMNLP_dataset/dialogues_emotion.txt  \n",
            "  inflating: EMNLP_dataset/dialogues_text.txt  \n",
            "  inflating: EMNLP_dataset/dialogues_topic.txt  \n",
            "  inflating: EMNLP_dataset/readme.txt  \n",
            "  inflating: __MACOSX/EMNLP_dataset/._readme.txt  \n",
            "  inflating: EMNLP_dataset/test.zip  \n",
            "  inflating: EMNLP_dataset/train.zip  \n",
            "  inflating: EMNLP_dataset/validation.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqk3W6u0yBCh",
        "outputId": "0275e32a-5070-49cb-a2c0-07f17007e877"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMNLP_dataset\t       __MACOSX\t\t\t      sample_data\n",
            "I17-1099.Datasets.zip  model.png\t\t      train-v2.0.json\n",
            "lstm_quora.hdf5        quora_duplicate_questions.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MVu1x5vxeUw"
      },
      "source": [
        "def load_daily_dialog(path_to_file:str) -> List[Tuple[str,str]]:\n",
        "    with open(f\"{path_to_file}/{DailyDialogKeys.ACT_FILENAME.value}\") as dialogue_act_file:\n",
        "        dialogue_acts = dialogue_act_file.readlines()\n",
        "\n",
        "    with open(f\"{path_to_file}/{DailyDialogKeys.DIALOGUE_FILENAME.value}\") as dialogue_file:\n",
        "        dialogues = dialogue_file.readlines()\n",
        "\n",
        "    for line_dialogue,line_acts in zip(dialogues,dialogue_acts):\n",
        "        question_in_dialogue = DailyDialogKeys.QUESTION_ACT.value in line_acts[1:]\n",
        "        if question_in_dialogue:\n",
        "            utterances = line_dialogue.split(DailyDialogKeys.END_OF_UTTERANCE.value)[:-1]\n",
        "            question_act_indexes = [\n",
        "                position for position, act_index in enumerate(line_acts.strip().split()) \\\n",
        "                if position > 0 and act_index == DailyDialogKeys.QUESTION_ACT.value\n",
        "            ]\n",
        "            for question_index in question_act_indexes:\n",
        "                question = utterances[question_index]\n",
        "                context = utterances[:question_index]\n",
        "                yield (' '.join(context),question)\n",
        "\n",
        "class DailyDialogKeys(Enum):\n",
        "    QUESTION_ACT = \"2\"\n",
        "    END_OF_UTTERANCE = \"__eou__\"\n",
        "    ACT_FILENAME = \"dialogues_act.txt\"\n",
        "    DIALOGUE_FILENAME = \"dialogues_text.txt\""
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXWLj8DExg9F"
      },
      "source": [
        "contexts,questions = zip(*load_daily_dialog(\"EMNLP_dataset\"))"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPF6HjojyNhh",
        "outputId": "30455366-6074-4299-c88a-a4dd953bbc0b"
      },
      "source": [
        "contexts[:3]"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Would you mind waiting a while ? ',\n",
              " 'Isn ’ t he the best instructor ? I think he ’ s so hot . Wow ! I really feel energized , don ’ t you ?   I swear , I ’ m going to kill you for this . ',\n",
              " 'Can I take your order now or do you still want to look at the menu ? ')"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o13Xwzq-yO3k",
        "outputId": "154dc85b-1205-4437-ef7a-1644f7df30d1"
      },
      "source": [
        "questions[:3]"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' Well , how long will it be ? ',\n",
              " ' What ’ s wrong ? Didn ’ t you think it was fun ? ! ',\n",
              " \" Well , I want a fillet steak , medium , but my little girl doesn't care for steak . Could she have something else instead ? \")"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S3ly3_GyXSN"
      },
      "source": [
        "I've included the trained model weights `lstm_dailydialog.hdf5` on my repo so that the model can be loaded and played around with (using `python interact_lstm.py` )  Enjoy!\n"
      ]
    }
  ]
}